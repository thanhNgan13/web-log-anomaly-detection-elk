##
# Logstash pipeline configuration
#
# This pipeline reads a CSV file from the `data` directory (mounted via the
# dockerâ€‘compose.yml) and indexes each line into Elasticsearch.  The CSV file
# must contain a header row with the field names defined below.
#
# Each log entry represents an HTTP request and contains the following
# columns:
#   timestamp, src_ip, dst_ip, method, url, status, response_time, bytes
#
# The `csv` filter parses the line into fields.  The `mutate` filter converts
# the numeric strings into integers/floats.  The `date` filter sets the
# `@timestamp` field used by Elasticsearch/Kibana to the log timestamp.

input {
  file {
    path => "/usr/share/logstash/pipeline/data/sample_logs.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  csv {
    separator => ","
    columns => ["timestamp","src_ip","dst_ip","method","url","status","response_time","bytes"]
  }
  mutate {
    convert => {
      "status" => "integer"
      "response_time" => "float"
      "bytes" => "integer"
    }
  }
  date {
    match => ["timestamp", "yyyy-MM-dd HH:mm:ss"]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "web-logs"
  }
  stdout {
    codec => rubydebug
  }
}